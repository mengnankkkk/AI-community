#!/usr/bin/env python3
"""
æ‰¹é‡è¯„ä¼°æ¼”ç¤ºç¨‹åº
ç”¨äºè¯„ä¼°ç”Ÿæˆçš„æ’­å®¢è´¨é‡ï¼Œç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
"""

import requests
import json
import os
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict
import time


class PodcastEvaluator:
    """æ’­å®¢è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, api_base_url: str = "http://localhost:8000"):
        self.api_base_url = api_base_url
    
    def evaluate_podcast(self, task_id: str) -> Dict:
        """è¯„ä¼°å•ä¸ªæ’­å®¢"""
        url = f"{self.api_base_url}/api/v1/quality/assess"
        
        try:
            payload = {
                "task_id": task_id,
                "include_audio_analysis": True,
                "include_content_analysis": True
            }
            
            response = requests.post(url, json=payload, timeout=60)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            return {"error": str(e), "success": False}
    
    def batch_evaluate(self, task_ids: List[str], output_dir: str = "output/evaluation"):
        """æ‰¹é‡è¯„ä¼°æ’­å®¢"""
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š æ‰¹é‡æ’­å®¢è´¨é‡è¯„ä¼°")
        print(f"{'='*60}")
        print(f"ğŸ¯ è¯„ä¼°ä»»åŠ¡æ•°: {len(task_ids)}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*60}\n")
        
        results = []
        
        for i, task_id in enumerate(task_ids, 1):
            print(f"[{i}/{len(task_ids)}] è¯„ä¼°ä»»åŠ¡: {task_id[:8]}...")
            
            start_time = time.time()
            evaluation = self.evaluate_podcast(task_id)
            elapsed_time = time.time() - start_time
            
            result = {
                "task_id": task_id,
                "evaluation_time": elapsed_time,
                "timestamp": datetime.now().isoformat()
            }
            
            if evaluation.get("success", True):
                result.update(evaluation)
                print(f"  âœ… è¯„ä¼°å®Œæˆ (è€—æ—¶: {elapsed_time:.1f}s)")
                
                # æ‰“å°å…³é”®æŒ‡æ ‡
                if "overall_score" in evaluation:
                    print(f"  ğŸ“Š ç»¼åˆè¯„åˆ†: {evaluation['overall_score']:.1f}/10")
                if "content_quality" in evaluation:
                    print(f"  ğŸ“ å†…å®¹è´¨é‡: {evaluation['content_quality']:.1f}/10")
                if "audio_quality" in evaluation:
                    print(f"  ğŸµ éŸ³é¢‘è´¨é‡: {evaluation['audio_quality']:.1f}/10")
            else:
                result["error"] = evaluation.get("error", "Unknown error")
                print(f"  âŒ è¯„ä¼°å¤±è´¥: {result['error']}")
            
            results.append(result)
            print()
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        self._save_detailed_report(results, output_dir)
        
        # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
        self._generate_summary_statistics(results, output_dir)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self._print_statistics(results)
        
        return results
    
    def _save_detailed_report(self, results: List[Dict], output_dir: str):
        """ä¿å­˜è¯¦ç»†è¯„ä¼°æŠ¥å‘Š"""
        report_path = os.path.join(output_dir, "evaluation_report.json")
        
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def _generate_summary_statistics(self, results: List[Dict], output_dir: str):
        """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡"""
        successful = [r for r in results if "error" not in r]
        
        if not successful:
            return
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        avg_overall = sum(r.get("overall_score", 0) for r in successful) / len(successful)
        avg_content = sum(r.get("content_quality", 0) for r in successful) / len(successful)
        avg_audio = sum(r.get("audio_quality", 0) for r in successful) / len(successful)
        
        summary = {
            "timestamp": datetime.now().isoformat(),
            "total_evaluated": len(results),
            "successful_evaluations": len(successful),
            "failed_evaluations": len(results) - len(successful),
            "average_scores": {
                "overall": round(avg_overall, 2),
                "content_quality": round(avg_content, 2),
                "audio_quality": round(avg_audio, 2)
            },
            "score_distribution": self._get_score_distribution(successful),
            "recommendations": self._generate_recommendations(successful)
        }
        
        summary_path = os.path.join(output_dir, "evaluation_summary.json")
        with open(summary_path, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ˆ æ±‡æ€»ç»Ÿè®¡å·²ä¿å­˜: {summary_path}")
    
    def _get_score_distribution(self, results: List[Dict]) -> Dict:
        """è·å–åˆ†æ•°åˆ†å¸ƒ"""
        distribution = {
            "excellent (9-10åˆ†)": 0,
            "good (7-8åˆ†)": 0,
            "acceptable (5-6åˆ†)": 0,
            "poor (<5åˆ†)": 0
        }
        
        for r in results:
            score = r.get("overall_score", 0)
            if score >= 9:
                distribution["excellent (9-10åˆ†)"] += 1
            elif score >= 7:
                distribution["good (7-8åˆ†)"] += 1
            elif score >= 5:
                distribution["acceptable (5-6åˆ†)"] += 1
            else:
                distribution["poor (<5åˆ†)"] += 1
        
        return distribution
    
    def _generate_recommendations(self, results: List[Dict]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        avg_content = sum(r.get("content_quality", 0) for r in results) / len(results)
        avg_audio = sum(r.get("audio_quality", 0) for r in results) / len(results)
        
        if avg_content < 7:
            recommendations.append("å»ºè®®ï¼šæå‡å†…å®¹è´¨é‡ï¼Œå¢å¼ºè¯é¢˜æ·±åº¦å’Œé€»è¾‘è¿è´¯æ€§")
        
        if avg_audio < 7:
            recommendations.append("å»ºè®®ï¼šä¼˜åŒ–éŸ³é¢‘å¤„ç†æµç¨‹ï¼Œæå‡è¯­éŸ³åˆæˆè´¨é‡")
        
        if avg_content >= 8 and avg_audio >= 8:
            recommendations.append("ä¼˜ç§€ï¼šæ•´ä½“è´¨é‡ä¼˜ç§€ï¼Œå¯ç›´æ¥ç”¨äºç”Ÿäº§ç¯å¢ƒ")
        
        return recommendations
    
    def _print_statistics(self, results: List[Dict]):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        successful = [r for r in results if "error" not in r]
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š è¯„ä¼°ç»Ÿè®¡æŠ¥å‘Š")
        print(f"{'='*60}")
        print(f"âœ… æˆåŠŸè¯„ä¼°: {len(successful)} / {len(results)}")
        print(f"âŒ å¤±è´¥è¯„ä¼°: {len(results) - len(successful)} / {len(results)}")
        
        if successful:
            avg_overall = sum(r.get("overall_score", 0) for r in successful) / len(successful)
            avg_content = sum(r.get("content_quality", 0) for r in successful) / len(successful)
            avg_audio = sum(r.get("audio_quality", 0) for r in successful) / len(successful)
            
            print(f"\nğŸ“ˆ å¹³å‡åˆ†æ•°:")
            print(f"  â€¢ ç»¼åˆè¯„åˆ†: {avg_overall:.2f}/10")
            print(f"  â€¢ å†…å®¹è´¨é‡: {avg_content:.2f}/10")
            print(f"  â€¢ éŸ³é¢‘è´¨é‡: {avg_audio:.2f}/10")
            
            # åˆ†æ•°åˆ†å¸ƒ
            distribution = self._get_score_distribution(successful)
            print(f"\nğŸ“Š åˆ†æ•°åˆ†å¸ƒ:")
            for level, count in distribution.items():
                percentage = count / len(successful) * 100
                print(f"  â€¢ {level}: {count} ({percentage:.1f}%)")
        
        print(f"{'='*60}\n")


def load_task_ids_from_report(report_path: str) -> List[str]:
    """ä»æ‰¹é‡ç”ŸæˆæŠ¥å‘Šä¸­åŠ è½½task_id"""
    with open(report_path, "r", encoding="utf-8") as f:
        report = json.load(f)
    
    task_ids = []
    for result in report.get("results", []):
        if result.get("status") == "completed":
            task_ids.append(result["task_id"])
    
    return task_ids


def main():
    parser = argparse.ArgumentParser(description="AIæ’­å®¢æ‰¹é‡è¯„ä¼°æ¼”ç¤ºç¨‹åº")
    parser.add_argument("--api", default="http://localhost:8000", help="APIæœåŠ¡åœ°å€")
    parser.add_argument("--output", default="output/evaluation", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--task-ids", nargs="+", help="è¦è¯„ä¼°çš„task_idåˆ—è¡¨")
    parser.add_argument("--report", help="æ‰¹é‡ç”ŸæˆæŠ¥å‘Šè·¯å¾„ (ä»ä¸­æå–task_id)")
    
    args = parser.parse_args()
    
    # è·å–task_idåˆ—è¡¨
    if args.task_ids:
        task_ids = args.task_ids
    elif args.report and os.path.exists(args.report):
        task_ids = load_task_ids_from_report(args.report)
        if not task_ids:
            print("âŒ æŠ¥å‘Šä¸­æœªæ‰¾åˆ°å·²å®Œæˆçš„ä»»åŠ¡")
            return
    else:
        print("âŒ è¯·æä¾› --task-ids æˆ– --report å‚æ•°")
        parser.print_help()
        return
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = PodcastEvaluator(api_base_url=args.api)
    
    # æ‰¹é‡è¯„ä¼°
    results = evaluator.batch_evaluate(task_ids, output_dir=args.output)
    
    print("âœ¨ æ‰¹é‡è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜è‡³: {args.output}")
    print(f"ğŸ“Š æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š: {os.path.join(args.output, 'evaluation_report.json')}")
    print(f"ğŸ“ˆ æŸ¥çœ‹æ±‡æ€»ç»Ÿè®¡: {os.path.join(args.output, 'evaluation_summary.json')}")


if __name__ == "__main__":
    main()
